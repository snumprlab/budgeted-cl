[=== Module anaconda/3 loaded ===]
[=== Module cudatoolkit/12.6.0 loaded ===]
/home/mila/s/sparsha.mishra/scratch/budgeted_CL_llava/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:datasets:PyTorch version 2.2.1+cu118 available.
[W CUDAAllocatorConfig.h:30] Warning: expandable_segments not supported on this platform (function operator())
/home/mila/s/sparsha.mishra/scratch/budgeted_CL_llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.45s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:02,  2.02s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  2.02s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.96s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.58s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:05<00:02,  2.75s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.25s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.27s/it]
/home/mila/s/sparsha.mishra/scratch/budgeted_CL_llava/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
  0%|          | 0/196 [00:00<?, ?it/s]  1%|          | 1/196 [00:10<33:40, 10.36s/it]  1%|          | 2/196 [00:11<16:24,  5.08s/it]                                                 1%|          | 2/196 [00:11<16:24,  5.08s/it]  2%|▏         | 3/196 [00:13<10:53,  3.39s/it]/network/scratch/s/sparsha.mishra/smh/budgeted_CL_LLAVA/utils/trainer.py:1975: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  y_hat = F.softmax(logit)
Traceback (most recent call last):
  File "/network/scratch/s/sparsha.mishra/smh/budgeted_CL_LLAVA/main_new_llava_trainer.py", line 382, in <module>
    main()
  File "/network/scratch/s/sparsha.mishra/smh/budgeted_CL_LLAVA/main_new_llava_trainer.py", line 291, in main
    results = trainer.train()
  File "/home/mila/s/sparsha.mishra/scratch/budgeted_CL_llava/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 361, in train
    output = super().train(*args, **kwargs)
  File "/home/mila/s/sparsha.mishra/scratch/budgeted_CL_llava/lib/python3.10/site-packages/transformers/trainer.py", line 1859, in train
    return inner_training_loop(
  File "/network/scratch/s/sparsha.mishra/smh/budgeted_CL_LLAVA/utils/trainer.py", line 2369, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/network/scratch/s/sparsha.mishra/smh/budgeted_CL_LLAVA/utils/trainer.py", line 1692, in training_step
    self.get_freeze_idx(logits, self.model.labels)
  File "/home/mila/s/sparsha.mishra/scratch/budgeted_CL_llava/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/network/scratch/s/sparsha.mishra/smh/budgeted_CL_LLAVA/utils/trainer.py", line 2000, in get_freeze_idx
    self.total_fisher - self.cumulative_fisher[i]) / (self.total_fisher + 1e-10))
AttributeError: 'CustomLLaVATrainer' object has no attribute 'total_fisher'
  2%|▏         | 3/196 [00:14<15:47,  4.91s/it]
